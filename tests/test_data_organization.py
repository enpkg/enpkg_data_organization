"""Test module to evaluate whether the current module of the ENPKG workflow."""
from .utils import retrieve_zenodo_data


def test_data_organization():
    """Test whether the data organization is correct."""
    # First, we retrieve the data from Zenodo if we have not done so already.
    retrieve_zenodo_data()

    # The files in the raw directory are open-MS files that contain
    # metadata associated to the samples
    # The processed folder contains the files that have been
    # manually processed by the user, that is the files 
    # that contain the peaks selected by the user.

    # XCMS pipeline for peak picking
    #     - Problem with the XCMS pipeline:
    #           - The files for the Earth metabolome are meant to be from several different machines, and therefore the parameters such as the noise would need to be adjusted for each machine.
    # For the time being we will assume that the processed data are 
    # provided by a machine expert user.

    # For each raw file we have a three corresponding files:
    #    - The mgf file is a text file with blocks, they encode fragmentation spectra for given features. Each file has a header with the metadata of the sample. In the list of numerical values there are two values per row. The first one is the mass divided by charge ratio of the fragment, while the second value is the intensity of fragment in the overall spectra. With these two lines it is possible to reconstruct the fragmentation spectra.
    #       - The feaure ID is a file-wise id for the feature.
    #       - Parent Ion Mass (PEPMASS) is the mass of the ion that was fragmented.
    #       - Scans is not used and we can ignore it at this time. It should generally be equal to the feature ID.
    #       - RTINSECONDS is the retention time of the feature. It is the time that the feature spent in the chromatography column.
    #       - CHARGE is the charge of the ion that was fragmented, usually it is 1+. **Investigate what is the difference between 1 and 1+.**
    #       - MSLEVEL is the level of the fragmentation spectra. In this case it is always 2, at least in this specific MGF file.
    # In the other MGF file with the longer name is annotated using Sirius, a pipeline to annotate MGF files. The MSLEVEL in the first block of a feature is 1 (full molecule), while the MSLEVEL in the second block is 2 (fragmented molecule).
    #   - If MSLEVEL is 1, we expect SPECTYPE=CORRELATED MS, and viceversa.
    #   - The smallest PEPMASS listed in the first block of features needs to be the smallest of the mass over charge in the first block of features. Both the first and second block associated to a feature must have the same PEPMASS and this value should match the smallest mass over charge value in the first block of features.
    # On the CSV file, which is a feature table. The header is expected to be: "row ID,row m/z,row retention time,{name of original raw file} Peak area,". These files are automatically generated by MZMine.
    #   - The row ID matches the feature ID
    #   - The row m/z matches the PEPMASS
    #   - The row retention time matches the RTINSECONDS, but in MINUTES. It has much less precision than the RTINSECONDS from the MGF file. This value, generally speaking, is quite noisy.
    #   - The last column, {name of original raw file} Peak area, is the intensity (area under the curve) of the feature of the current sample.
    # All of the values in the columns have to be strictly positives larger than zero. The first column should contain values ranging from 0 to 2000. For the second column, the retention time from 0 to 15 minutes. Finally for the third column, we can expect values ranging from 0 to 10^9. We would want for any sample metadata associated to the machine used to analyse it.
    # We can use outlier detection to identify potential machine errors.
    # 
    # The metadata TSV file is curated by hand, so it needs to be severely validated. The meaning of
    # these columns is currently listed in the README of the repository. The TSV document may also contain
    # several other additional columns that are ignored at this time.
    #
    # - The sample_id column is the unique identifier of the sample. It looks like: dbgi_000083_01_01, and should be pattern validated. Look up the paper available at URL https://account.datascience.codata.org/index.php/up-j-dsj/article/view/dsj-2021-011/1055 in the future.
    # - The sample_filename_pos have to been historically unique, but the sample ID is not necessarily unique as one may execute multiple runs on the same sample.
    # - sample type has to be `QC` (quality control), `blank` (only solvant) or `sample`.
    # - source_id is at this time the prefix of the sample_id
    # - The id column is the iNaturalist observation ID, so it means that there should be an online resolvable URL for each of the samples. The IDs look like 112079170.
    #      - The iNaturalist data updates every now and then, so it is reasonable to crawl the website periodically.
    #      - We need to ask to iNaturalist people whether they plan to offer full website dumps to convert it into a KG for ETL
    # 
    # The file dbgi_tropical_toydataset_lcms_params.txt encodes the parameter used when running the MS analysis
    # The dbgi_tropical_toydataset_mzmine_params.xml contains the parameters used by the software MZMine to process the data.